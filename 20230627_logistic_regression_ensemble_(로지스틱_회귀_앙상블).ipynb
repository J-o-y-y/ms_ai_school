{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsX2StgKhRyOBky53TZIxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-o-y-y/ms_ai_school/blob/main/20230627_logistic_regression_ensemble_(%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80_%EC%95%99%EC%83%81%EB%B8%94).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 로지스틱 희귀 앙상블 실습\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "gYDmhhPmINEJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 데이터셋, 데이터 로더 생성\n",
        "dataset = FashionMNIST(root=\"./data\", train=True, transform=ToTensor(), download=True)\n",
        "train_set, val_set = train_test_split(dataset, test_size=0.1, random_state=777)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(val_set, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnX0wyNlIOf_",
        "outputId": "1c44aac8-69d3-4b44-fa2c-5f44e475ac57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 6913985.91it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 118777.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:02<00:00, 2169502.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 14510938.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 모델 선언\n",
        "class LogisticRegression(nn.Module) :\n",
        "    def __init__(self, input_size, num_classes) :\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x) :\n",
        "        out = self.linear(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Rm6ThWdBIiQd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 하이퍼 파라미터 설정\n",
        "input_size = 28 * 28\n",
        "num_classes = 10\n",
        "num_epoch = 100\n",
        "lr =  0.001\n",
        "num_models = 5 # 앙상블에 사용할 모델 개수"
      ],
      "metadata": {
        "id": "xPn-6gDzIv_G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 앙상블\n",
        "models = [LogisticRegression(input_size, num_classes) for _ in range(num_models)]\n",
        "print(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j823fSZI05A",
        "outputId": "02d04733-4470-4b1a-f716-2ffc688c3f91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            "), LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            "), LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            "), LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            "), LogisticRegression(\n",
            "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            ")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 모델, 손실 함수, 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizers = [optim.SGD(model.parameters(), lr=lr) for model in models]\n",
        "print(optimizers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEEF0njBI3Ws",
        "outputId": "f092a0d2-e71f-41cb-e320-03861216db48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            "), SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztAvkcfKH2Gq",
        "outputId": "1ddcdfe6-7d14-472c-9ac1-3c5d94f0afbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Val ACC >> 65.43%\n",
            "Epoch [2/100], Val ACC >> 66.97%\n",
            "Epoch [3/100], Val ACC >> 68.05%\n",
            "Epoch [4/100], Val ACC >> 68.97%\n",
            "Epoch [5/100], Val ACC >> 70.33%\n",
            "Epoch [6/100], Val ACC >> 71.90%\n",
            "Epoch [7/100], Val ACC >> 73.08%\n",
            "Epoch [8/100], Val ACC >> 73.80%\n",
            "Epoch [9/100], Val ACC >> 74.45%\n",
            "Epoch [10/100], Val ACC >> 75.08%\n",
            "Epoch [11/100], Val ACC >> 75.58%\n",
            "Epoch [12/100], Val ACC >> 76.08%\n",
            "Epoch [13/100], Val ACC >> 76.63%\n",
            "Epoch [14/100], Val ACC >> 77.00%\n",
            "Epoch [15/100], Val ACC >> 77.43%\n",
            "Epoch [16/100], Val ACC >> 77.63%\n",
            "Epoch [17/100], Val ACC >> 77.87%\n",
            "Epoch [18/100], Val ACC >> 78.05%\n",
            "Epoch [19/100], Val ACC >> 78.35%\n",
            "Epoch [20/100], Val ACC >> 78.68%\n",
            "Epoch [21/100], Val ACC >> 78.80%\n",
            "Epoch [22/100], Val ACC >> 79.08%\n",
            "Epoch [23/100], Val ACC >> 79.32%\n",
            "Epoch [24/100], Val ACC >> 79.48%\n",
            "Epoch [25/100], Val ACC >> 79.53%\n",
            "Epoch [26/100], Val ACC >> 79.75%\n",
            "Epoch [27/100], Val ACC >> 79.78%\n",
            "Epoch [28/100], Val ACC >> 80.00%\n",
            "Epoch [29/100], Val ACC >> 80.03%\n",
            "Epoch [30/100], Val ACC >> 80.10%\n",
            "Epoch [31/100], Val ACC >> 80.20%\n",
            "Epoch [32/100], Val ACC >> 80.43%\n",
            "Epoch [33/100], Val ACC >> 80.37%\n",
            "Epoch [34/100], Val ACC >> 80.53%\n",
            "Epoch [35/100], Val ACC >> 80.57%\n",
            "Epoch [36/100], Val ACC >> 80.73%\n",
            "Epoch [37/100], Val ACC >> 80.78%\n",
            "Epoch [38/100], Val ACC >> 80.83%\n",
            "Epoch [39/100], Val ACC >> 80.97%\n",
            "Epoch [40/100], Val ACC >> 81.12%\n",
            "Epoch [41/100], Val ACC >> 81.08%\n",
            "Epoch [42/100], Val ACC >> 81.15%\n",
            "Epoch [43/100], Val ACC >> 81.18%\n",
            "Epoch [44/100], Val ACC >> 81.38%\n",
            "Epoch [45/100], Val ACC >> 81.37%\n",
            "Epoch [46/100], Val ACC >> 81.57%\n",
            "Epoch [47/100], Val ACC >> 81.62%\n",
            "Epoch [48/100], Val ACC >> 81.57%\n",
            "Epoch [49/100], Val ACC >> 81.83%\n",
            "Epoch [50/100], Val ACC >> 81.88%\n",
            "Epoch [51/100], Val ACC >> 81.82%\n",
            "Epoch [52/100], Val ACC >> 81.85%\n",
            "Epoch [53/100], Val ACC >> 81.97%\n",
            "Epoch [54/100], Val ACC >> 82.10%\n",
            "Epoch [55/100], Val ACC >> 82.05%\n",
            "Epoch [56/100], Val ACC >> 82.20%\n",
            "Epoch [57/100], Val ACC >> 82.20%\n",
            "Epoch [58/100], Val ACC >> 82.33%\n",
            "Epoch [59/100], Val ACC >> 82.42%\n",
            "Epoch [60/100], Val ACC >> 82.37%\n",
            "Epoch [61/100], Val ACC >> 82.45%\n",
            "Epoch [62/100], Val ACC >> 82.50%\n",
            "Epoch [63/100], Val ACC >> 82.55%\n",
            "Epoch [64/100], Val ACC >> 82.68%\n",
            "Epoch [65/100], Val ACC >> 82.65%\n",
            "Epoch [66/100], Val ACC >> 82.62%\n",
            "Epoch [67/100], Val ACC >> 82.58%\n",
            "Epoch [68/100], Val ACC >> 82.73%\n",
            "Epoch [69/100], Val ACC >> 82.80%\n",
            "Epoch [70/100], Val ACC >> 82.78%\n",
            "Epoch [71/100], Val ACC >> 82.82%\n",
            "Epoch [72/100], Val ACC >> 82.88%\n",
            "Epoch [73/100], Val ACC >> 82.97%\n",
            "Epoch [74/100], Val ACC >> 82.93%\n",
            "Epoch [75/100], Val ACC >> 82.90%\n",
            "Epoch [76/100], Val ACC >> 82.95%\n",
            "Epoch [77/100], Val ACC >> 83.02%\n",
            "Epoch [78/100], Val ACC >> 82.98%\n",
            "Epoch [79/100], Val ACC >> 83.07%\n",
            "Epoch [80/100], Val ACC >> 83.17%\n",
            "Epoch [81/100], Val ACC >> 83.23%\n",
            "Epoch [82/100], Val ACC >> 83.20%\n",
            "Epoch [83/100], Val ACC >> 83.23%\n",
            "Epoch [84/100], Val ACC >> 83.30%\n",
            "Epoch [85/100], Val ACC >> 83.28%\n",
            "Epoch [86/100], Val ACC >> 83.28%\n",
            "Epoch [87/100], Val ACC >> 83.30%\n",
            "Epoch [88/100], Val ACC >> 83.35%\n",
            "Epoch [89/100], Val ACC >> 83.35%\n",
            "Epoch [90/100], Val ACC >> 83.43%\n",
            "Epoch [91/100], Val ACC >> 83.42%\n",
            "Epoch [92/100], Val ACC >> 83.45%\n",
            "Epoch [93/100], Val ACC >> 83.45%\n",
            "Epoch [94/100], Val ACC >> 83.48%\n",
            "Epoch [95/100], Val ACC >> 83.43%\n",
            "Epoch [96/100], Val ACC >> 83.60%\n",
            "Epoch [97/100], Val ACC >> 83.63%\n",
            "Epoch [98/100], Val ACC >> 83.60%\n",
            "Epoch [99/100], Val ACC >> 83.60%\n",
            "Epoch [100/100], Val ACC >> 83.55%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "### train loop\n",
        "for epoch in range(num_epoch) :\n",
        "    for i, (images, labels) in enumerate(train_loader) :\n",
        "        # 데이터 로드\n",
        "        images = images.reshape(-1, input_size)\n",
        "        labels = labels\n",
        "\n",
        "        # 순전파 및 손실 계산\n",
        "        for j in range(num_models) :\n",
        "            outputs = models[j](images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "           # 역전파 및 가중치 업데이트\n",
        "            optimizers[j].zero_grad()\n",
        "            loss.backward()\n",
        "            optimizers[j].step()\n",
        "\n",
        "    # 검증 코드 추가 #\n",
        "    with torch.no_grad() :\n",
        "        total, correct = 0,0\n",
        "        for images, labels in test_loader :\n",
        "            images = images.reshape(-1, input_size)\n",
        "            \"\"\"\n",
        "            outputs = torch.zeros(images.size()[0], num_classes)\n",
        "            이미지 배치에 대한 출력 텐서 초기화\n",
        "            >> 후속 단계에서 이미지에 대한 예측값 업데이트 가능\n",
        "            \"\"\"\n",
        "            outputs = torch.zeros(images.size()[0], num_classes)\n",
        "            # 앙상블 모델의 예측값 더하기\n",
        "            for j in range(num_models) :\n",
        "                outputs += models[j](images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epoch}], Val ACC >> {val_acc:.2f}%\")\n"
      ]
    }
  ]
}