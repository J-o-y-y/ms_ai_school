{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBGwKz5PoU14MPzWXiqxO9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-o-y-y/ms_ai_school/blob/main/20230626_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80_%EB%AA%A8%EB%8D%B8%EC%9D%98_%ED%95%99%EC%8A%B5%EC%97%90%EC%84%9C_%EB%8B%A4%EC%96%91%ED%95%9C_%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80%EB%A5%BC_%EC%A0%81%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SVvBnWRQFBTG"
      },
      "outputs": [],
      "source": [
        "# 선형 회귀 모델의 학습에서 다양한 옵티마이저를 적용해보기\n",
        "# BostonHousing.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import Standardscaler\n",
        "from sklearn.model.selection import train_test_split"
      ],
      "metadata": {
        "id": "IJGEIcbBFFGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(data, columns=[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"])\n",
        "df[\"target\"] = target\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br0JBVkDFpkY",
        "outputId": "41415b44-e8c4-4cf9-c7bb-e41c66593178"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
            "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
            "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
            "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
            "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
            "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
            "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
            "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
            "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
            "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
            "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
            "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
            "\n",
            "     PTRATIO       B  LSTAT  target  \n",
            "0       15.3  396.90   4.98    24.0  \n",
            "1       17.8  396.90   9.14    21.6  \n",
            "2       17.8  392.83   4.03    34.7  \n",
            "3       18.7  394.63   2.94    33.4  \n",
            "4       18.7  396.90   5.33    36.2  \n",
            "..       ...     ...    ...     ...  \n",
            "501     21.0  391.99   9.67    22.4  \n",
            "502     21.0  396.90   9.08    20.6  \n",
            "503     21.0  396.90   5.64    23.9  \n",
            "504     21.0  393.45   6.48    22.0  \n",
            "505     21.0  396.90   7.88    11.9  \n",
            "\n",
            "[506 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n"
      ],
      "metadata": {
        "id": "eTFp6EBHIqj7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 데이터 불러오기\n",
        "data = pd.read_csv('BostonHousing.csv')\n",
        "\n",
        "# 입력 변수와 출력 변수 나누기\n",
        "X = data.drop('MEDV', axis=1)\n",
        "y = data['MEDV']\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# 선형 회귀 모델 학습\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 평가: 평균 제곱 오차 계산\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqmxJClwJ7kb",
        "outputId": "90e1297a-3c40-42ec-920e-0c8d46b4ede2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 11.064023538881518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성 및 하이퍼파라미터 설정"
      ],
      "metadata": {
        "id": "n7tlt3ZvGpsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_dim = x_data.shape[1]  # 입력 차원\n",
        "output_dim = 1  # 출력 차원\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "4x2JmHFEKgCE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.001  # 학습률\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n"
      ],
      "metadata": {
        "id": "J8DyFuihKmvX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "HW-n_gf1I7-q"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = raw_df.values[:, :-1]\n",
        "target = raw_df.values[:, -1]\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.001\n",
        "epochs = 2000\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# 다양한 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n",
        "\n",
        "# 학습\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(\"Optimizer:\", optimizer_name)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VSfxmjpJmam",
        "outputId": "f8e68cb8-d514-4653-ab6d-a5b2f8f2ce1e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: SGD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([910])) that is different to the input size (torch.Size([910, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - EPOCH [100/2000], Loss: nan\n",
            "SGD - EPOCH [200/2000], Loss: nan\n",
            "SGD - EPOCH [300/2000], Loss: nan\n",
            "SGD - EPOCH [400/2000], Loss: nan\n",
            "SGD - EPOCH [500/2000], Loss: nan\n",
            "SGD - EPOCH [600/2000], Loss: nan\n",
            "SGD - EPOCH [700/2000], Loss: nan\n",
            "SGD - EPOCH [800/2000], Loss: nan\n",
            "SGD - EPOCH [900/2000], Loss: nan\n",
            "SGD - EPOCH [1000/2000], Loss: nan\n",
            "SGD - EPOCH [1100/2000], Loss: nan\n",
            "SGD - EPOCH [1200/2000], Loss: nan\n",
            "SGD - EPOCH [1300/2000], Loss: nan\n",
            "SGD - EPOCH [1400/2000], Loss: nan\n",
            "SGD - EPOCH [1500/2000], Loss: nan\n",
            "SGD - EPOCH [1600/2000], Loss: nan\n",
            "SGD - EPOCH [1700/2000], Loss: nan\n",
            "SGD - EPOCH [1800/2000], Loss: nan\n",
            "SGD - EPOCH [1900/2000], Loss: nan\n",
            "SGD - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Momentum\n",
            "Momentum - EPOCH [100/2000], Loss: nan\n",
            "Momentum - EPOCH [200/2000], Loss: nan\n",
            "Momentum - EPOCH [300/2000], Loss: nan\n",
            "Momentum - EPOCH [400/2000], Loss: nan\n",
            "Momentum - EPOCH [500/2000], Loss: nan\n",
            "Momentum - EPOCH [600/2000], Loss: nan\n",
            "Momentum - EPOCH [700/2000], Loss: nan\n",
            "Momentum - EPOCH [800/2000], Loss: nan\n",
            "Momentum - EPOCH [900/2000], Loss: nan\n",
            "Momentum - EPOCH [1000/2000], Loss: nan\n",
            "Momentum - EPOCH [1100/2000], Loss: nan\n",
            "Momentum - EPOCH [1200/2000], Loss: nan\n",
            "Momentum - EPOCH [1300/2000], Loss: nan\n",
            "Momentum - EPOCH [1400/2000], Loss: nan\n",
            "Momentum - EPOCH [1500/2000], Loss: nan\n",
            "Momentum - EPOCH [1600/2000], Loss: nan\n",
            "Momentum - EPOCH [1700/2000], Loss: nan\n",
            "Momentum - EPOCH [1800/2000], Loss: nan\n",
            "Momentum - EPOCH [1900/2000], Loss: nan\n",
            "Momentum - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adagrad\n",
            "Adagrad - EPOCH [100/2000], Loss: nan\n",
            "Adagrad - EPOCH [200/2000], Loss: nan\n",
            "Adagrad - EPOCH [300/2000], Loss: nan\n",
            "Adagrad - EPOCH [400/2000], Loss: nan\n",
            "Adagrad - EPOCH [500/2000], Loss: nan\n",
            "Adagrad - EPOCH [600/2000], Loss: nan\n",
            "Adagrad - EPOCH [700/2000], Loss: nan\n",
            "Adagrad - EPOCH [800/2000], Loss: nan\n",
            "Adagrad - EPOCH [900/2000], Loss: nan\n",
            "Adagrad - EPOCH [1000/2000], Loss: nan\n",
            "Adagrad - EPOCH [1100/2000], Loss: nan\n",
            "Adagrad - EPOCH [1200/2000], Loss: nan\n",
            "Adagrad - EPOCH [1300/2000], Loss: nan\n",
            "Adagrad - EPOCH [1400/2000], Loss: nan\n",
            "Adagrad - EPOCH [1500/2000], Loss: nan\n",
            "Adagrad - EPOCH [1600/2000], Loss: nan\n",
            "Adagrad - EPOCH [1700/2000], Loss: nan\n",
            "Adagrad - EPOCH [1800/2000], Loss: nan\n",
            "Adagrad - EPOCH [1900/2000], Loss: nan\n",
            "Adagrad - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: RMSprop\n",
            "RMSprop - EPOCH [100/2000], Loss: nan\n",
            "RMSprop - EPOCH [200/2000], Loss: nan\n",
            "RMSprop - EPOCH [300/2000], Loss: nan\n",
            "RMSprop - EPOCH [400/2000], Loss: nan\n",
            "RMSprop - EPOCH [500/2000], Loss: nan\n",
            "RMSprop - EPOCH [600/2000], Loss: nan\n",
            "RMSprop - EPOCH [700/2000], Loss: nan\n",
            "RMSprop - EPOCH [800/2000], Loss: nan\n",
            "RMSprop - EPOCH [900/2000], Loss: nan\n",
            "RMSprop - EPOCH [1000/2000], Loss: nan\n",
            "RMSprop - EPOCH [1100/2000], Loss: nan\n",
            "RMSprop - EPOCH [1200/2000], Loss: nan\n",
            "RMSprop - EPOCH [1300/2000], Loss: nan\n",
            "RMSprop - EPOCH [1400/2000], Loss: nan\n",
            "RMSprop - EPOCH [1500/2000], Loss: nan\n",
            "RMSprop - EPOCH [1600/2000], Loss: nan\n",
            "RMSprop - EPOCH [1700/2000], Loss: nan\n",
            "RMSprop - EPOCH [1800/2000], Loss: nan\n",
            "RMSprop - EPOCH [1900/2000], Loss: nan\n",
            "RMSprop - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adam\n",
            "Adam - EPOCH [100/2000], Loss: nan\n",
            "Adam - EPOCH [200/2000], Loss: nan\n",
            "Adam - EPOCH [300/2000], Loss: nan\n",
            "Adam - EPOCH [400/2000], Loss: nan\n",
            "Adam - EPOCH [500/2000], Loss: nan\n",
            "Adam - EPOCH [600/2000], Loss: nan\n",
            "Adam - EPOCH [700/2000], Loss: nan\n",
            "Adam - EPOCH [800/2000], Loss: nan\n",
            "Adam - EPOCH [900/2000], Loss: nan\n",
            "Adam - EPOCH [1000/2000], Loss: nan\n",
            "Adam - EPOCH [1100/2000], Loss: nan\n",
            "Adam - EPOCH [1200/2000], Loss: nan\n",
            "Adam - EPOCH [1300/2000], Loss: nan\n",
            "Adam - EPOCH [1400/2000], Loss: nan\n",
            "Adam - EPOCH [1500/2000], Loss: nan\n",
            "Adam - EPOCH [1600/2000], Loss: nan\n",
            "Adam - EPOCH [1700/2000], Loss: nan\n",
            "Adam - EPOCH [1800/2000], Loss: nan\n",
            "Adam - EPOCH [1900/2000], Loss: nan\n",
            "Adam - EPOCH [2000/2000], Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv(\"BostonHousing.csv\")\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "data = df.drop(\"MEDV\", axis=1).values\n",
        "target = df[\"MEDV\"].values\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.001\n",
        "epochs = 2000\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# 다양한 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n",
        "\n",
        "# 학습\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(\"Optimizer:\", optimizer_name)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kWkyoHBOHHG",
        "outputId": "72f03762-5b84-416a-d0a1-8dc14cb3af11"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: SGD\n",
            "SGD - EPOCH [100/2000], Loss: 123.9977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - EPOCH [200/2000], Loss: 473.8532\n",
            "SGD - EPOCH [300/2000], Loss: 330.7986\n",
            "SGD - EPOCH [400/2000], Loss: 235.9974\n",
            "SGD - EPOCH [500/2000], Loss: 543.6885\n",
            "SGD - EPOCH [600/2000], Loss: 94.5323\n",
            "SGD - EPOCH [700/2000], Loss: 581.7615\n",
            "SGD - EPOCH [800/2000], Loss: 170.7771\n",
            "SGD - EPOCH [900/2000], Loss: 409.8972\n",
            "SGD - EPOCH [1000/2000], Loss: 398.3912\n",
            "SGD - EPOCH [1100/2000], Loss: 180.1285\n",
            "SGD - EPOCH [1200/2000], Loss: 578.0265\n",
            "SGD - EPOCH [1300/2000], Loss: 90.3826\n",
            "SGD - EPOCH [1400/2000], Loss: 553.4513\n",
            "SGD - EPOCH [1500/2000], Loss: 226.0558\n",
            "SGD - EPOCH [1600/2000], Loss: 337.6535\n",
            "SGD - EPOCH [1700/2000], Loss: 468.6103\n",
            "SGD - EPOCH [1800/2000], Loss: 129.8642\n",
            "SGD - EPOCH [1900/2000], Loss: 594.8562\n",
            "SGD - EPOCH [2000/2000], Loss: 112.3570\n",
            "Optimizer: Momentum\n",
            "Momentum - EPOCH [100/2000], Loss: 6136.0737\n",
            "Momentum - EPOCH [200/2000], Loss: 292116070400.0000\n",
            "Momentum - EPOCH [300/2000], Loss: 803994032152772608.0000\n",
            "Momentum - EPOCH [400/2000], Loss: 14377506470278180607361024.0000\n",
            "Momentum - EPOCH [500/2000], Loss: 194863290500134700372932150951936.0000\n",
            "Momentum - EPOCH [600/2000], Loss: inf\n",
            "Momentum - EPOCH [700/2000], Loss: inf\n",
            "Momentum - EPOCH [800/2000], Loss: inf\n",
            "Momentum - EPOCH [900/2000], Loss: inf\n",
            "Momentum - EPOCH [1000/2000], Loss: inf\n",
            "Momentum - EPOCH [1100/2000], Loss: inf\n",
            "Momentum - EPOCH [1200/2000], Loss: nan\n",
            "Momentum - EPOCH [1300/2000], Loss: nan\n",
            "Momentum - EPOCH [1400/2000], Loss: nan\n",
            "Momentum - EPOCH [1500/2000], Loss: nan\n",
            "Momentum - EPOCH [1600/2000], Loss: nan\n",
            "Momentum - EPOCH [1700/2000], Loss: nan\n",
            "Momentum - EPOCH [1800/2000], Loss: nan\n",
            "Momentum - EPOCH [1900/2000], Loss: nan\n",
            "Momentum - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adagrad\n",
            "Adagrad - EPOCH [100/2000], Loss: nan\n",
            "Adagrad - EPOCH [200/2000], Loss: nan\n",
            "Adagrad - EPOCH [300/2000], Loss: nan\n",
            "Adagrad - EPOCH [400/2000], Loss: nan\n",
            "Adagrad - EPOCH [500/2000], Loss: nan\n",
            "Adagrad - EPOCH [600/2000], Loss: nan\n",
            "Adagrad - EPOCH [700/2000], Loss: nan\n",
            "Adagrad - EPOCH [800/2000], Loss: nan\n",
            "Adagrad - EPOCH [900/2000], Loss: nan\n",
            "Adagrad - EPOCH [1000/2000], Loss: nan\n",
            "Adagrad - EPOCH [1100/2000], Loss: nan\n",
            "Adagrad - EPOCH [1200/2000], Loss: nan\n",
            "Adagrad - EPOCH [1300/2000], Loss: nan\n",
            "Adagrad - EPOCH [1400/2000], Loss: nan\n",
            "Adagrad - EPOCH [1500/2000], Loss: nan\n",
            "Adagrad - EPOCH [1600/2000], Loss: nan\n",
            "Adagrad - EPOCH [1700/2000], Loss: nan\n",
            "Adagrad - EPOCH [1800/2000], Loss: nan\n",
            "Adagrad - EPOCH [1900/2000], Loss: nan\n",
            "Adagrad - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: RMSprop\n",
            "RMSprop - EPOCH [100/2000], Loss: nan\n",
            "RMSprop - EPOCH [200/2000], Loss: nan\n",
            "RMSprop - EPOCH [300/2000], Loss: nan\n",
            "RMSprop - EPOCH [400/2000], Loss: nan\n",
            "RMSprop - EPOCH [500/2000], Loss: nan\n",
            "RMSprop - EPOCH [600/2000], Loss: nan\n",
            "RMSprop - EPOCH [700/2000], Loss: nan\n",
            "RMSprop - EPOCH [800/2000], Loss: nan\n",
            "RMSprop - EPOCH [900/2000], Loss: nan\n",
            "RMSprop - EPOCH [1000/2000], Loss: nan\n",
            "RMSprop - EPOCH [1100/2000], Loss: nan\n",
            "RMSprop - EPOCH [1200/2000], Loss: nan\n",
            "RMSprop - EPOCH [1300/2000], Loss: nan\n",
            "RMSprop - EPOCH [1400/2000], Loss: nan\n",
            "RMSprop - EPOCH [1500/2000], Loss: nan\n",
            "RMSprop - EPOCH [1600/2000], Loss: nan\n",
            "RMSprop - EPOCH [1700/2000], Loss: nan\n",
            "RMSprop - EPOCH [1800/2000], Loss: nan\n",
            "RMSprop - EPOCH [1900/2000], Loss: nan\n",
            "RMSprop - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adam\n",
            "Adam - EPOCH [100/2000], Loss: nan\n",
            "Adam - EPOCH [200/2000], Loss: nan\n",
            "Adam - EPOCH [300/2000], Loss: nan\n",
            "Adam - EPOCH [400/2000], Loss: nan\n",
            "Adam - EPOCH [500/2000], Loss: nan\n",
            "Adam - EPOCH [600/2000], Loss: nan\n",
            "Adam - EPOCH [700/2000], Loss: nan\n",
            "Adam - EPOCH [800/2000], Loss: nan\n",
            "Adam - EPOCH [900/2000], Loss: nan\n",
            "Adam - EPOCH [1000/2000], Loss: nan\n",
            "Adam - EPOCH [1100/2000], Loss: nan\n",
            "Adam - EPOCH [1200/2000], Loss: nan\n",
            "Adam - EPOCH [1300/2000], Loss: nan\n",
            "Adam - EPOCH [1400/2000], Loss: nan\n",
            "Adam - EPOCH [1500/2000], Loss: nan\n",
            "Adam - EPOCH [1600/2000], Loss: nan\n",
            "Adam - EPOCH [1700/2000], Loss: nan\n",
            "Adam - EPOCH [1800/2000], Loss: nan\n",
            "Adam - EPOCH [1900/2000], Loss: nan\n",
            "Adam - EPOCH [2000/2000], Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv(\"BostonHousing.csv\")\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "data = df.drop(\"MEDV\", axis=1).values\n",
        "target = df[\"MEDV\"].values\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.001\n",
        "epochs = 2000\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# 다양한 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n",
        "\n",
        "# 학습\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(\"Optimizer:\", optimizer_name)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tjc2N8LOam4",
        "outputId": "23350f50-e5b6-4ee2-cc4b-f98ae3263aef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: SGD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - EPOCH [100/2000], Loss: 124.8475\n",
            "SGD - EPOCH [200/2000], Loss: 484.5770\n",
            "SGD - EPOCH [300/2000], Loss: 337.2220\n",
            "SGD - EPOCH [400/2000], Loss: 240.1667\n",
            "SGD - EPOCH [500/2000], Loss: 556.2567\n",
            "SGD - EPOCH [600/2000], Loss: 94.5572\n",
            "SGD - EPOCH [700/2000], Loss: 595.4426\n",
            "SGD - EPOCH [800/2000], Loss: 172.9345\n",
            "SGD - EPOCH [900/2000], Loss: 418.6797\n",
            "SGD - EPOCH [1000/2000], Loss: 406.8661\n",
            "SGD - EPOCH [1100/2000], Loss: 182.6711\n",
            "SGD - EPOCH [1200/2000], Loss: 591.3251\n",
            "SGD - EPOCH [1300/2000], Loss: 90.6173\n",
            "SGD - EPOCH [1400/2000], Loss: 566.1469\n",
            "SGD - EPOCH [1500/2000], Loss: 229.8172\n",
            "SGD - EPOCH [1600/2000], Loss: 344.5999\n",
            "SGD - EPOCH [1700/2000], Loss: 478.9474\n",
            "SGD - EPOCH [1800/2000], Loss: 130.8677\n",
            "SGD - EPOCH [1900/2000], Loss: 608.8223\n",
            "SGD - EPOCH [2000/2000], Loss: 112.8906\n",
            "Optimizer: Momentum\n",
            "Momentum - EPOCH [100/2000], Loss: 9808.6035\n",
            "Momentum - EPOCH [200/2000], Loss: 8309442084864.0000\n",
            "Momentum - EPOCH [300/2000], Loss: 22870903985973231616.0000\n",
            "Momentum - EPOCH [400/2000], Loss: 408989477473057299155124224.0000\n",
            "Momentum - EPOCH [500/2000], Loss: inf\n",
            "Momentum - EPOCH [600/2000], Loss: inf\n",
            "Momentum - EPOCH [700/2000], Loss: inf\n",
            "Momentum - EPOCH [800/2000], Loss: inf\n",
            "Momentum - EPOCH [900/2000], Loss: inf\n",
            "Momentum - EPOCH [1000/2000], Loss: inf\n",
            "Momentum - EPOCH [1100/2000], Loss: nan\n",
            "Momentum - EPOCH [1200/2000], Loss: nan\n",
            "Momentum - EPOCH [1300/2000], Loss: nan\n",
            "Momentum - EPOCH [1400/2000], Loss: nan\n",
            "Momentum - EPOCH [1500/2000], Loss: nan\n",
            "Momentum - EPOCH [1600/2000], Loss: nan\n",
            "Momentum - EPOCH [1700/2000], Loss: nan\n",
            "Momentum - EPOCH [1800/2000], Loss: nan\n",
            "Momentum - EPOCH [1900/2000], Loss: nan\n",
            "Momentum - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adagrad\n",
            "Adagrad - EPOCH [100/2000], Loss: nan\n",
            "Adagrad - EPOCH [200/2000], Loss: nan\n",
            "Adagrad - EPOCH [300/2000], Loss: nan\n",
            "Adagrad - EPOCH [400/2000], Loss: nan\n",
            "Adagrad - EPOCH [500/2000], Loss: nan\n",
            "Adagrad - EPOCH [600/2000], Loss: nan\n",
            "Adagrad - EPOCH [700/2000], Loss: nan\n",
            "Adagrad - EPOCH [800/2000], Loss: nan\n",
            "Adagrad - EPOCH [900/2000], Loss: nan\n",
            "Adagrad - EPOCH [1000/2000], Loss: nan\n",
            "Adagrad - EPOCH [1100/2000], Loss: nan\n",
            "Adagrad - EPOCH [1200/2000], Loss: nan\n",
            "Adagrad - EPOCH [1300/2000], Loss: nan\n",
            "Adagrad - EPOCH [1400/2000], Loss: nan\n",
            "Adagrad - EPOCH [1500/2000], Loss: nan\n",
            "Adagrad - EPOCH [1600/2000], Loss: nan\n",
            "Adagrad - EPOCH [1700/2000], Loss: nan\n",
            "Adagrad - EPOCH [1800/2000], Loss: nan\n",
            "Adagrad - EPOCH [1900/2000], Loss: nan\n",
            "Adagrad - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: RMSprop\n",
            "RMSprop - EPOCH [100/2000], Loss: nan\n",
            "RMSprop - EPOCH [200/2000], Loss: nan\n",
            "RMSprop - EPOCH [300/2000], Loss: nan\n",
            "RMSprop - EPOCH [400/2000], Loss: nan\n",
            "RMSprop - EPOCH [500/2000], Loss: nan\n",
            "RMSprop - EPOCH [600/2000], Loss: nan\n",
            "RMSprop - EPOCH [700/2000], Loss: nan\n",
            "RMSprop - EPOCH [800/2000], Loss: nan\n",
            "RMSprop - EPOCH [900/2000], Loss: nan\n",
            "RMSprop - EPOCH [1000/2000], Loss: nan\n",
            "RMSprop - EPOCH [1100/2000], Loss: nan\n",
            "RMSprop - EPOCH [1200/2000], Loss: nan\n",
            "RMSprop - EPOCH [1300/2000], Loss: nan\n",
            "RMSprop - EPOCH [1400/2000], Loss: nan\n",
            "RMSprop - EPOCH [1500/2000], Loss: nan\n",
            "RMSprop - EPOCH [1600/2000], Loss: nan\n",
            "RMSprop - EPOCH [1700/2000], Loss: nan\n",
            "RMSprop - EPOCH [1800/2000], Loss: nan\n",
            "RMSprop - EPOCH [1900/2000], Loss: nan\n",
            "RMSprop - EPOCH [2000/2000], Loss: nan\n",
            "Optimizer: Adam\n",
            "Adam - EPOCH [100/2000], Loss: nan\n",
            "Adam - EPOCH [200/2000], Loss: nan\n",
            "Adam - EPOCH [300/2000], Loss: nan\n",
            "Adam - EPOCH [400/2000], Loss: nan\n",
            "Adam - EPOCH [500/2000], Loss: nan\n",
            "Adam - EPOCH [600/2000], Loss: nan\n",
            "Adam - EPOCH [700/2000], Loss: nan\n",
            "Adam - EPOCH [800/2000], Loss: nan\n",
            "Adam - EPOCH [900/2000], Loss: nan\n",
            "Adam - EPOCH [1000/2000], Loss: nan\n",
            "Adam - EPOCH [1100/2000], Loss: nan\n",
            "Adam - EPOCH [1200/2000], Loss: nan\n",
            "Adam - EPOCH [1300/2000], Loss: nan\n",
            "Adam - EPOCH [1400/2000], Loss: nan\n",
            "Adam - EPOCH [1500/2000], Loss: nan\n",
            "Adam - EPOCH [1600/2000], Loss: nan\n",
            "Adam - EPOCH [1700/2000], Loss: nan\n",
            "Adam - EPOCH [1800/2000], Loss: nan\n",
            "Adam - EPOCH [1900/2000], Loss: nan\n",
            "Adam - EPOCH [2000/2000], Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv(\"BostonHousing.csv\")\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "data = df.drop(\"MEDV\", axis=1).values\n",
        "target = df[\"MEDV\"].values\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# 다양한 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n",
        "\n",
        "# 학습\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(\"Optimizer:\", optimizer_name)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv5Z9VPjO-aJ",
        "outputId": "74d81a21-c666-4b54-b09f-d1fdbf51ea1a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: SGD\n",
            "SGD - EPOCH [100/1000], Loss: 124.6499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - EPOCH [200/1000], Loss: 468.0185\n",
            "SGD - EPOCH [300/1000], Loss: 327.2032\n",
            "SGD - EPOCH [400/1000], Loss: 233.5929\n",
            "SGD - EPOCH [500/1000], Loss: 535.8404\n",
            "SGD - EPOCH [600/1000], Loss: 94.8149\n",
            "SGD - EPOCH [700/1000], Loss: 573.9581\n",
            "SGD - EPOCH [800/1000], Loss: 170.6338\n",
            "SGD - EPOCH [900/1000], Loss: 405.7191\n",
            "SGD - EPOCH [1000/1000], Loss: 394.5131\n",
            "Optimizer: Momentum\n",
            "Momentum - EPOCH [100/1000], Loss: 46256.7344\n",
            "Momentum - EPOCH [200/1000], Loss: 8578130944.0000\n",
            "Momentum - EPOCH [300/1000], Loss: 23473944040308736.0000\n",
            "Momentum - EPOCH [400/1000], Loss: 419753463640642690220032.0000\n",
            "Momentum - EPOCH [500/1000], Loss: 5689229085622837188750758379520.0000\n",
            "Momentum - EPOCH [600/1000], Loss: inf\n",
            "Momentum - EPOCH [700/1000], Loss: inf\n",
            "Momentum - EPOCH [800/1000], Loss: inf\n",
            "Momentum - EPOCH [900/1000], Loss: inf\n",
            "Momentum - EPOCH [1000/1000], Loss: inf\n",
            "Optimizer: Adagrad\n",
            "Adagrad - EPOCH [100/1000], Loss: inf\n",
            "Adagrad - EPOCH [200/1000], Loss: inf\n",
            "Adagrad - EPOCH [300/1000], Loss: inf\n",
            "Adagrad - EPOCH [400/1000], Loss: inf\n",
            "Adagrad - EPOCH [500/1000], Loss: inf\n",
            "Adagrad - EPOCH [600/1000], Loss: inf\n",
            "Adagrad - EPOCH [700/1000], Loss: inf\n",
            "Adagrad - EPOCH [800/1000], Loss: inf\n",
            "Adagrad - EPOCH [900/1000], Loss: inf\n",
            "Adagrad - EPOCH [1000/1000], Loss: inf\n",
            "Optimizer: RMSprop\n",
            "RMSprop - EPOCH [100/1000], Loss: inf\n",
            "RMSprop - EPOCH [200/1000], Loss: inf\n",
            "RMSprop - EPOCH [300/1000], Loss: inf\n",
            "RMSprop - EPOCH [400/1000], Loss: inf\n",
            "RMSprop - EPOCH [500/1000], Loss: inf\n",
            "RMSprop - EPOCH [600/1000], Loss: inf\n",
            "RMSprop - EPOCH [700/1000], Loss: inf\n",
            "RMSprop - EPOCH [800/1000], Loss: inf\n",
            "RMSprop - EPOCH [900/1000], Loss: inf\n",
            "RMSprop - EPOCH [1000/1000], Loss: inf\n",
            "Optimizer: Adam\n",
            "Adam - EPOCH [100/1000], Loss: inf\n",
            "Adam - EPOCH [200/1000], Loss: inf\n",
            "Adam - EPOCH [300/1000], Loss: inf\n",
            "Adam - EPOCH [400/1000], Loss: inf\n",
            "Adam - EPOCH [500/1000], Loss: inf\n",
            "Adam - EPOCH [600/1000], Loss: inf\n",
            "Adam - EPOCH [700/1000], Loss: inf\n",
            "Adam - EPOCH [800/1000], Loss: inf\n",
            "Adam - EPOCH [900/1000], Loss: inf\n",
            "Adam - EPOCH [1000/1000], Loss: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv(\"BostonHousing.csv\")\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "data = df.drop(\"MEDV\", axis=1).values\n",
        "target = df[\"MEDV\"].values\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "\n",
        "# 모델 생성\n",
        "model = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "# 다양한 옵티마이저 설정\n",
        "optimizers = {\n",
        "    \"SGD\": optim.SGD(model.parameters(), lr=lr),\n",
        "    \"Momentum\": optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "    \"Adagrad\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "    \"RMSprop\": optim.RMSprop(model.parameters(), lr=lr),\n",
        "    \"Adam\": optim.Adam(model.parameters(), lr=lr)\n",
        "}\n",
        "\n",
        "# 학습\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "    print(\"Optimizer:\", optimizer_name)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # NaN 또는 Inf 손실 확인\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: NaN or Inf\")\n",
        "            break\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑 (그래디언트 폭주 방지)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # 진행 상황 출력\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"{optimizer_name} - EPOCH [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgc7CuWUPtP7",
        "outputId": "3d238dd6-3b4b-4efc-912b-a69e628d916c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: SGD\n",
            "SGD - EPOCH [100/1000], Loss: 601.0172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - EPOCH [200/1000], Loss: 596.4852\n",
            "SGD - EPOCH [300/1000], Loss: 591.9734\n",
            "SGD - EPOCH [400/1000], Loss: 587.4818\n",
            "SGD - EPOCH [500/1000], Loss: 583.0103\n",
            "SGD - EPOCH [600/1000], Loss: 578.5589\n",
            "SGD - EPOCH [700/1000], Loss: 574.1275\n",
            "SGD - EPOCH [800/1000], Loss: 569.7164\n",
            "SGD - EPOCH [900/1000], Loss: 565.3253\n",
            "SGD - EPOCH [1000/1000], Loss: 560.9544\n",
            "Optimizer: Momentum\n",
            "Momentum - EPOCH [100/1000], Loss: 522.4793\n",
            "Momentum - EPOCH [200/1000], Loss: 481.6876\n",
            "Momentum - EPOCH [300/1000], Loss: 442.9045\n",
            "Momentum - EPOCH [400/1000], Loss: 406.1292\n",
            "Momentum - EPOCH [500/1000], Loss: 371.3607\n",
            "Momentum - EPOCH [600/1000], Loss: 338.5981\n",
            "Momentum - EPOCH [700/1000], Loss: 307.8409\n",
            "Momentum - EPOCH [800/1000], Loss: 279.0883\n",
            "Momentum - EPOCH [900/1000], Loss: 252.3395\n",
            "Momentum - EPOCH [1000/1000], Loss: 227.5936\n",
            "Optimizer: Adagrad\n",
            "Adagrad - EPOCH [100/1000], Loss: 226.8881\n",
            "Adagrad - EPOCH [200/1000], Loss: 226.6808\n",
            "Adagrad - EPOCH [300/1000], Loss: 226.5227\n",
            "Adagrad - EPOCH [400/1000], Loss: 226.3899\n",
            "Adagrad - EPOCH [500/1000], Loss: 226.2733\n",
            "Adagrad - EPOCH [600/1000], Loss: 226.1681\n",
            "Adagrad - EPOCH [700/1000], Loss: 226.0716\n",
            "Adagrad - EPOCH [800/1000], Loss: 225.9821\n",
            "Adagrad - EPOCH [900/1000], Loss: 225.8980\n",
            "Adagrad - EPOCH [1000/1000], Loss: 225.8187\n",
            "Optimizer: RMSprop\n",
            "RMSprop - EPOCH [100/1000], Loss: 221.0191\n",
            "RMSprop - EPOCH [200/1000], Loss: 218.3563\n",
            "RMSprop - EPOCH [300/1000], Loss: 215.9539\n",
            "RMSprop - EPOCH [400/1000], Loss: 213.6465\n",
            "RMSprop - EPOCH [500/1000], Loss: 211.3868\n",
            "RMSprop - EPOCH [600/1000], Loss: 209.1584\n",
            "RMSprop - EPOCH [700/1000], Loss: 206.9546\n",
            "RMSprop - EPOCH [800/1000], Loss: 204.7720\n",
            "RMSprop - EPOCH [900/1000], Loss: 202.6095\n",
            "RMSprop - EPOCH [1000/1000], Loss: 200.4669\n",
            "Optimizer: Adam\n",
            "Adam - EPOCH [100/1000], Loss: 198.3443\n",
            "Adam - EPOCH [200/1000], Loss: 196.2417\n",
            "Adam - EPOCH [300/1000], Loss: 194.1591\n",
            "Adam - EPOCH [400/1000], Loss: 192.0964\n",
            "Adam - EPOCH [500/1000], Loss: 190.0537\n",
            "Adam - EPOCH [600/1000], Loss: 188.0311\n",
            "Adam - EPOCH [700/1000], Loss: 186.0283\n",
            "Adam - EPOCH [800/1000], Loss: 184.0455\n",
            "Adam - EPOCH [900/1000], Loss: 182.0827\n",
            "Adam - EPOCH [1000/1000], Loss: 180.1399\n"
          ]
        }
      ]
    }
  ]
}